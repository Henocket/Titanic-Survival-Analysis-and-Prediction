{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75617437",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "Decision tree is a popular and simple machine learning algorithm.  The algorithm determines a result based on each feature provided at multiple tree levels.  Every node in a tree signifies a choice.  The node will then either have a leaf node, which is a final choice, or a branch with more nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b5347de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the processed and splitted data\n",
    "\n",
    "xtrain = \"data/X_train.csv\"\n",
    "X_train = pd.read_csv(xtrain)\n",
    "\n",
    "xtest = \"data/X_test.csv\"\n",
    "X_test = pd.read_csv(xtest)\n",
    "\n",
    "ytrain = \"data/y_train.csv\"\n",
    "y_train = pd.read_csv(ytrain)\n",
    "\n",
    "ytest = \"data/y_test.csv\"\n",
    "y_test = pd.read_csv(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e25910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81       105\n",
      "           1       0.74      0.70      0.72        74\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.77      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the data into the Decision tree classifier\n",
    "# Get predictions and evaluate the result\n",
    "\n",
    "treeCls = DecisionTreeClassifier()\n",
    "treeCls = treeCls.fit(X_train, y_train)\n",
    "\n",
    "prediction = treeCls.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e223c",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "A random forest is another machine learning that uses trees for classification and regression. It is works by combining multiple decision tree which are trained no multiple subset of features instead of all at the same time. The final decision is made by majority vote of the decision trees for classification or by averaging out the trees outcome for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b60da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88       105\n",
      "           1       0.84      0.82      0.83        74\n",
      "\n",
      "    accuracy                           0.86       179\n",
      "   macro avg       0.86      0.86      0.86       179\n",
      "weighted avg       0.86      0.86      0.86       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the data into the Random Forest classifier\n",
    "# Get predictions and evaluate the result\n",
    "\n",
    "rdmFrst = RandomForestClassifier()\n",
    "rdmFrst = rdmFrst.fit(X_train, y_train.to_numpy().ravel())\n",
    "prediction = rdmFrst.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99499b8",
   "metadata": {},
   "source": [
    "The Random Forest model significantly outperformed the Decision Tree. This improvement can be attributed to Random Forest's use of multiple decision trees trained on different subsets of the data. By aggregating the results from various trees, Random Forest reduces the overfitting problem that is often seen in individual Decision Trees, leading to better generalization on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1247fa5",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classification\n",
    "\n",
    "The linear SVC algorithm classifies data by finding a hyperplane that best separates the data into different classes. The hyperplane depends on critical points that are closest, known as support vectors. Each data instances can be represented as a point in space based on their features. The hyperplane generated by the algorithm will separate the instances, effectively classifying the dataset. The decision boundary, i.e the hyperplane, is directly dependant on the support vectors. The linear SVC is an optimized SVC algorithm for speed and scalabilty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d647dd7",
   "metadata": {},
   "source": [
    "## Traditional SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b2415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.47      0.52       105\n",
      "           1       0.40      0.51      0.45        74\n",
      "\n",
      "    accuracy                           0.49       179\n",
      "   macro avg       0.49      0.49      0.48       179\n",
      "weighted avg       0.51      0.49      0.49       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tradSvc = SVC(class_weight='balanced')\n",
    "tradSvc = tradSvc.fit(X_train, y_train.to_numpy().ravel())\n",
    "prediction = tradSvc.predict(X_test)\n",
    "print(classification_report(y_test, prediction, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7bc79",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eca6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       105\n",
      "           1       0.78      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.78      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linSvc = LinearSVC()\n",
    "linSvc = linSvc.fit(X_train, y_train.to_numpy().ravel())\n",
    "prediction = linSvc.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3bed2",
   "metadata": {},
   "source": [
    "The linear SVC model greatly outperforms the traditional SVC algorithm. This is due to poor generalization on the traditional algorithm. The linear algorithm perfroms well on unbalanced data compared to the former."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633c00a",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Classifier\n",
    "\n",
    "The KNN algorithm predicts based on distance among data points. The algorithm remembers all of the training data, then when a new data point is introduced, it calculates the distance to each point from the training set and classifies the new point to whichever n nearest points' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4fd146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68       105\n",
      "           1       0.47      0.30      0.36        74\n",
      "\n",
      "    accuracy                           0.57       179\n",
      "   macro avg       0.54      0.53      0.52       179\n",
      "weighted avg       0.55      0.57      0.55       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn = knn.fit(X_train, y_train.to_numpy().ravel())\n",
    "prediction = knn.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3a6c4",
   "metadata": {},
   "source": [
    "##  Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes is a classification algorithm based on Bayes’ Theorem with the assumption that the features follow a normal (Gaussian) distribution. If features are continuous, we assume that the likelihood of the features given the class follows a Gaussian (normal) distribution. . It calculates the probability of each class given the input features by combining the prior probability of each class with the likelihood of the features, modeled using a Gaussian distribution. The class with the highest posterior probability is chosen as the predicted class. It's fast, efficient, and works well with continuous data, even with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576e931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80       105\n",
      "           1       0.70      0.82      0.76        74\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.79      0.78       179\n",
      "weighted avg       0.79      0.78      0.78       179\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henoc\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "gaus = GaussianNB()\n",
    "gaus = gaus.fit(X_train, y_train)\n",
    "prediction = gaus.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3f1db",
   "metadata": {},
   "source": [
    "## CatBoost Classifier\n",
    "\n",
    "CatBoost Classifier is a high-performance, open-source machine learning algorithm based on gradient boosting. Gradient Boosting is a powerful machine learning technique used for both classification and regression problems. It builds an ensemble of decision trees in a sequential manner, where each new tree tries to correct the errors made by the previous ones. Instead of simply averaging the outputs like in random forests, gradient boosting fits the new tree to the residuals (the differences between predicted and actual values) of the previous trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411d022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
